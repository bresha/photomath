{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert images into grayscale, invert colors and dilate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = \"../data_copy/\"\n",
    "\n",
    "dirList = os.listdir(dataDir)\n",
    "\n",
    "for directory in dirList:\n",
    "    path = os.path.join(dataDir, directory)\n",
    "\n",
    "    for fileName in os.listdir(path):\n",
    "        imagePath = os.path.join(path, fileName)\n",
    "\n",
    "        image = cv2.imread(imagePath)\n",
    "\n",
    "        grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        inverted = cv2.bitwise_not(grayscale)\n",
    "\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "\n",
    "        dilatedImage = cv2.dilate(inverted, kernel, iterations=1)\n",
    "\n",
    "        folder = os.path.join(dataDir, directory)\n",
    "\n",
    "        imagePath = folder + \"/\" + fileName\n",
    "\n",
    "        cv2.imwrite(imagePath, dilatedImage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 185465 files belonging to 16 classes.\n",
      "Using 148372 files for training.\n",
      "Found 185465 files belonging to 16 classes.\n",
      "Using 37093 files for validation.\n"
     ]
    }
   ],
   "source": [
    "ds_train = keras.utils.image_dataset_from_directory(\n",
    "    directory = \"../data_copy/\",\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=64,\n",
    "    image_size=(45, 45),\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "ds_val = keras.utils.image_dataset_from_directory(\n",
    "    directory = \"../data_copy/\",\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=64,\n",
    "    image_size=(45, 45),\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "ds_test = ds_val.take(145)\n",
    "ds_val = ds_val.skip(145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches for testing --> tf.Tensor(145, shape=(), dtype=int64)\n",
      "Batches for validating --> tf.Tensor(435, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print('Batches for testing -->', ds_test.cardinality())\n",
    "print('Batches for validating -->', ds_val.cardinality())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_height = 45\n",
    "img_width = 45\n",
    "num_classes = 16\n",
    "img_channels = 1\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu',input_shape=(img_height, img_width, img_channels)),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2319/2319 [==============================] - 10s 4ms/step - loss: 0.2101 - accuracy: 0.9557 - val_loss: 0.0579 - val_accuracy: 0.9812\n",
      "Epoch 2/15\n",
      "2319/2319 [==============================] - 9s 4ms/step - loss: 0.0573 - accuracy: 0.9820 - val_loss: 0.0412 - val_accuracy: 0.9861\n",
      "Epoch 3/15\n",
      "2319/2319 [==============================] - 9s 4ms/step - loss: 0.0465 - accuracy: 0.9848 - val_loss: 0.0347 - val_accuracy: 0.9887\n",
      "Epoch 4/15\n",
      "2319/2319 [==============================] - 9s 4ms/step - loss: 0.0394 - accuracy: 0.9872 - val_loss: 0.0314 - val_accuracy: 0.9895\n",
      "Epoch 5/15\n",
      "2319/2319 [==============================] - 9s 4ms/step - loss: 0.0339 - accuracy: 0.9889 - val_loss: 0.0252 - val_accuracy: 0.9909\n",
      "Epoch 6/15\n",
      "2319/2319 [==============================] - 9s 4ms/step - loss: 0.0298 - accuracy: 0.9904 - val_loss: 0.0210 - val_accuracy: 0.9931\n",
      "Epoch 7/15\n",
      "2319/2319 [==============================] - 9s 4ms/step - loss: 0.0271 - accuracy: 0.9910 - val_loss: 0.0198 - val_accuracy: 0.9938\n",
      "Epoch 8/15\n",
      "2319/2319 [==============================] - 9s 4ms/step - loss: 0.0237 - accuracy: 0.9923 - val_loss: 0.0206 - val_accuracy: 0.9944\n",
      "Epoch 9/15\n",
      "2319/2319 [==============================] - 9s 4ms/step - loss: 0.0218 - accuracy: 0.9933 - val_loss: 0.0164 - val_accuracy: 0.9954\n",
      "Epoch 10/15\n",
      "2319/2319 [==============================] - 9s 4ms/step - loss: 0.0207 - accuracy: 0.9935 - val_loss: 0.0130 - val_accuracy: 0.9963\n",
      "Epoch 11/15\n",
      "2319/2319 [==============================] - 9s 4ms/step - loss: 0.0189 - accuracy: 0.9942 - val_loss: 0.0146 - val_accuracy: 0.9961\n",
      "Epoch 12/15\n",
      "2319/2319 [==============================] - 10s 4ms/step - loss: 0.0185 - accuracy: 0.9945 - val_loss: 0.0130 - val_accuracy: 0.9967\n",
      "Epoch 13/15\n",
      "2319/2319 [==============================] - 10s 4ms/step - loss: 0.0193 - accuracy: 0.9942 - val_loss: 0.0135 - val_accuracy: 0.9963\n",
      "Epoch 14/15\n",
      "2319/2319 [==============================] - 10s 4ms/step - loss: 0.0157 - accuracy: 0.9953 - val_loss: 0.0170 - val_accuracy: 0.9954\n",
      "Epoch 15/15\n",
      "2319/2319 [==============================] - 10s 4ms/step - loss: 0.0171 - accuracy: 0.9953 - val_loss: 0.0123 - val_accuracy: 0.9969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77657dbfa0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=15, validation_data=ds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 0.9964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.012755334377288818, 0.9964439868927002]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 17:13:55.594677: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/model1/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"../models/model1\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4043f920f50c1fdf9a9b8b4290230a8c58ae9e914d50003ffd089c1c19a29f7f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cvenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
