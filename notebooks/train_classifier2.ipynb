{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert images into grayscale, invert colors and dilate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = \"../data_copy_copy/\"\n",
    "\n",
    "dirList = os.listdir(dataDir)\n",
    "\n",
    "for directory in dirList:\n",
    "    path = os.path.join(dataDir, directory)\n",
    "\n",
    "    for fileName in os.listdir(path):\n",
    "        imagePath = os.path.join(path, fileName)\n",
    "\n",
    "        image = cv2.imread(imagePath)\n",
    "\n",
    "        grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "\n",
    "        dilatedImage = cv2.dilate(grayscale, kernel, iterations=1)\n",
    "\n",
    "        folder = os.path.join(dataDir, directory)\n",
    "\n",
    "        imagePath = folder + \"/\" + fileName + \"again.jpg\"\n",
    "\n",
    "        cv2.imwrite(imagePath, dilatedImage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 370930 files belonging to 16 classes.\n",
      "Using 296744 files for training.\n",
      "Found 370930 files belonging to 16 classes.\n",
      "Using 74186 files for validation.\n"
     ]
    }
   ],
   "source": [
    "ds_train = keras.utils.image_dataset_from_directory(\n",
    "    directory = \"../data_copy_copy/\",\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=64,\n",
    "    image_size=(45, 45),\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "ds_val = keras.utils.image_dataset_from_directory(\n",
    "    directory = \"../data_copy_copy/\",\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=64,\n",
    "    image_size=(45, 45),\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "ds_test = ds_val.take(290)\n",
    "ds_val = ds_val.skip(290)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches for testing --> tf.Tensor(290, shape=(), dtype=int64)\n",
      "Batches for validating --> tf.Tensor(870, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print('Batches for testing -->', ds_test.cardinality())\n",
    "print('Batches for validating -->', ds_val.cardinality())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_height = 45\n",
    "img_width = 45\n",
    "num_classes = 16\n",
    "img_channels = 1\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu',input_shape=(img_height, img_width, img_channels)),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 18:13:38.886167: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4637/4637 [==============================] - 36s 7ms/step - loss: 0.1528 - accuracy: 0.9657 - val_loss: 0.0531 - val_accuracy: 0.9832\n",
      "Epoch 2/15\n",
      "4637/4637 [==============================] - 22s 5ms/step - loss: 0.0515 - accuracy: 0.9834 - val_loss: 0.0358 - val_accuracy: 0.9891\n",
      "Epoch 3/15\n",
      "4637/4637 [==============================] - 23s 5ms/step - loss: 0.0407 - accuracy: 0.9867 - val_loss: 0.0366 - val_accuracy: 0.9888\n",
      "Epoch 4/15\n",
      "4637/4637 [==============================] - 21s 5ms/step - loss: 0.0348 - accuracy: 0.9885 - val_loss: 0.0287 - val_accuracy: 0.9904\n",
      "Epoch 5/15\n",
      "4637/4637 [==============================] - 20s 4ms/step - loss: 0.0317 - accuracy: 0.9897 - val_loss: 0.0254 - val_accuracy: 0.9924\n",
      "Epoch 6/15\n",
      "4637/4637 [==============================] - 20s 4ms/step - loss: 0.0285 - accuracy: 0.9906 - val_loss: 0.0235 - val_accuracy: 0.9922\n",
      "Epoch 7/15\n",
      "4637/4637 [==============================] - 20s 4ms/step - loss: 0.0265 - accuracy: 0.9914 - val_loss: 0.0210 - val_accuracy: 0.9933\n",
      "Epoch 8/15\n",
      "4637/4637 [==============================] - 20s 4ms/step - loss: 0.0244 - accuracy: 0.9920 - val_loss: 0.0225 - val_accuracy: 0.9934\n",
      "Epoch 9/15\n",
      "4637/4637 [==============================] - 20s 4ms/step - loss: 0.0231 - accuracy: 0.9927 - val_loss: 0.0199 - val_accuracy: 0.9930\n",
      "Epoch 10/15\n",
      "4637/4637 [==============================] - 21s 4ms/step - loss: 0.0221 - accuracy: 0.9930 - val_loss: 0.0191 - val_accuracy: 0.9939\n",
      "Epoch 11/15\n",
      "4637/4637 [==============================] - 21s 4ms/step - loss: 0.0216 - accuracy: 0.9932 - val_loss: 0.0162 - val_accuracy: 0.9943\n",
      "Epoch 12/15\n",
      "4637/4637 [==============================] - 21s 4ms/step - loss: 0.0221 - accuracy: 0.9934 - val_loss: 0.0186 - val_accuracy: 0.9941\n",
      "Epoch 13/15\n",
      "4637/4637 [==============================] - 21s 4ms/step - loss: 0.0204 - accuracy: 0.9938 - val_loss: 0.0191 - val_accuracy: 0.9941\n",
      "Epoch 14/15\n",
      "4637/4637 [==============================] - 21s 4ms/step - loss: 0.0209 - accuracy: 0.9941 - val_loss: 0.0216 - val_accuracy: 0.9933\n",
      "Epoch 15/15\n",
      "4637/4637 [==============================] - 21s 4ms/step - loss: 0.0210 - accuracy: 0.9935 - val_loss: 0.0176 - val_accuracy: 0.9951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f74efee26a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=15, validation_data=ds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - 1s 3ms/step - loss: 0.0152 - accuracy: 0.9956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015204863622784615, 0.9955819249153137]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 18:19:16.125794: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/model2/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"../models/model2\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4043f920f50c1fdf9a9b8b4290230a8c58ae9e914d50003ffd089c1c19a29f7f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cvenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
